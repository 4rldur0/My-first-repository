{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0dda287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pixellib.semantic import semantic_segmentation\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e64ce3ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32/1202202003.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimg_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "img_path = os.getenv('HOME')+'/aiffel/human_segmentation/images/my_second_image.png'  \n",
    "img_orig = cv2.imread(img_path) \n",
    "\n",
    "print(img_orig.shape)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(img_orig, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769fcd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = os.getenv('HOME')+'/aiffel/human_segmentation/models' \n",
    "model_file = os.path.join(model_dir, 'deeplabv3_xception_tf_dim_ordering_tf_kernels.h5') \n",
    "model_url = 'https://github.com/ayoolaolafenwa/PixelLib/releases/download/1.1/deeplabv3_xception_tf_dim_ordering_tf_kernels.h5' \n",
    "\n",
    "urllib.request.urlretrieve(model_url, model_file) # urllib 패키지 내에 있는 request 모듈의 urlretrieve 함수를 이용해서 model_url에 있는 파일을 다운로드 해서 model_file 파일명으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae337d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = semantic_segmentation() #PixelLib 라이브러리 에서 가져온 클래스를 가져와서 semantic segmentation을 수행하는 클래스 인스턴스를 만듬\n",
    "model.load_pascalvoc_model(model_file) # pascal voc에 대해 훈련된 예외 모델(model_file)을 로드하는 함수를 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b3d026",
   "metadata": {},
   "outputs": [],
   "source": [
    "segvalues, output = model.segmentAsPascalvoc(img_path) \n",
    "# segmentAsPascalvoc()함 수 를 호출 하여 입력된 이미지를 분할, 분할 출력의 배열을 가져옴, 분할 은 pacalvoc 데이터로 학습된 모델을 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b3f2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pascalvoc 데이터의 라벨종류\n",
    "LABEL_NAMES = [\n",
    "    'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    "    'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
    "    'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tv'\n",
    "]\n",
    "len(LABEL_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e935c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#segmentAsPascalvoc() 함수 를 호출하여 입력된 이미지를 분할한 뒤 나온 결과값 중 output을 matplotlib을 이용해 출력\n",
    "plt.imshow(output)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f565f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "segvalues # segmentAsPascalvoc() 함수를 호출하여 입력된 이미지를 분할한 뒤 나온 결과값 중 배열값을 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961816fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#segvalues에 있는 class_ids를 담겨있는 값을 통해 pacalvoc에 담겨있는 라벨을 출력\n",
    "for class_id in segvalues['class_ids']:\n",
    "    print(LABEL_NAMES[class_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bd7590",
   "metadata": {},
   "outputs": [],
   "source": [
    "#컬러맵 만들기 \n",
    "colormap = np.zeros((256, 3), dtype = int)\n",
    "ind = np.arange(256, dtype=int)\n",
    "\n",
    "for shift in reversed(range(8)):\n",
    "    for channel in range(3):\n",
    "        colormap[:, channel] |= ((ind >> channel) & 1) << shift\n",
    "    ind >>= 3\n",
    "\n",
    "colormap[:20] #생성한 20개의 컬러맵 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5ad899",
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap[15] #컬러맵 15에 해당하는 배열 출력 (pacalvoc에 LABEL_NAMES 15번째인 사람)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d8bfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_color = (128,128,192) # 색상순서 변경 - colormap의 배열은 RGB 순이며 output의 배열은 BGR 순서로 채널 배치가 되어 있어서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d09f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output의 픽셀 별로 색상이 seg_color와 같다면 1(True), 다르다면 0(False)이 됩니다\n",
    "# seg_color 값이 person을 값이 므로 사람이 있는 위치를 제외하고는 gray로 출력\n",
    "# cmap 값을 변경하면 다른 색상으로 확인이 가능함\n",
    "seg_map = np.all(output==seg_color, axis=-1) \n",
    "print(seg_map.shape) \n",
    "plt.imshow(seg_map, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db3c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문제점 해결 방안\n",
    "# Semantic segmentation에서 피사계 심도를 표현하는 아웃포커싱 효과를 구현하는 데는 한계가 있는데, 특히 경계가 불명확한 영역에서 그렇습니다. \n",
    "# 깊이 예측(Depth Estimation)과 함께 semantic segmentation을 결합하여 응용하는 방식으로 예시를 들어 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287321b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# MiDaS 모델 불러오기\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS\")\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
    "\n",
    "# DeepLabV3 모델 불러오기\n",
    "deeplab = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet101', pretrained=True)\n",
    "deeplab.eval()\n",
    "\n",
    "# 이미지 불러오기\n",
    "img = cv2.imread(\"my_second_image.jpg\")\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# 깊이 예측\n",
    "input_batch = midas_transforms(img_rgb).unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    prediction = midas(input_batch)\n",
    "    prediction = torch.nn.functional.interpolate(\n",
    "        prediction.unsqueeze(1),\n",
    "        size=img_rgb.shape[:2],\n",
    "        mode=\"bicubic\",\n",
    "        align_corners=False,\n",
    "    ).squeeze()\n",
    "depth_map = prediction.cpu().numpy()\n",
    "\n",
    "# Semantic segmentation 예측\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(520),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_tensor = preprocess(img_rgb)\n",
    "input_batch = input_tensor.unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = deeplab(input_batch)['out'][0]\n",
    "output_predictions = output.argmax(0).byte().cpu().numpy()\n",
    "\n",
    "# 깊이 맵과 세그멘테이션 결과 결합\n",
    "def combine_depth_and_segmentation(depth, segmentation, threshold=0.5):\n",
    "    combined = np.zeros_like(segmentation)\n",
    "    for i in range(segmentation.max() + 1):\n",
    "        mask = (segmentation == i)\n",
    "        depth_values = depth[mask]\n",
    "        if depth_values.mean() > threshold:\n",
    "            combined[mask] = i\n",
    "    return combined\n",
    "\n",
    "combined_result = combine_depth_and_segmentation(depth_map, output_predictions)\n",
    "\n",
    "# 결과 시각화\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "ax[0].imshow(depth_map, cmap='plasma')\n",
    "ax[0].set_title('Depth Map')\n",
    "ax[1].imshow(output_predictions)\n",
    "ax[1].set_title('Segmentation')\n",
    "ax[2].imshow(combined_result)\n",
    "ax[2].set_title('Combined Result')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c6c35c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
